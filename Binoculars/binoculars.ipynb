{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a02930",
   "metadata": {},
   "source": [
    "# Binoculars: Zero-Shot AI Text Detection\n",
    "\n",
    "This notebook implements the **Binoculars** method for detecting AI-generated text, as described in the paper [Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text](https://arxiv.org/pdf/2401.12070).\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "Binoculars uses **two LLMs** (an \"observer\" and a \"performer\") to detect AI-generated text without requiring any training:\n",
    "\n",
    "- **Observer**: A chat/instruct-tuned model (e.g., Llama-3.2-1B-Instruct)\n",
    "- **Performer**: The base version of the same model (e.g., Llama-3.2-1B)\n",
    "\n",
    "The method computes:\n",
    "1. **PPL (Perplexity)**: How surprised the observer model is by the text\n",
    "2. **X-PPL (Cross-Perplexity)**: How surprised the observer is by what the performer predicts\n",
    "\n",
    "**The insight**: AI-generated text tends to have X-PPL/PPL ratio close to 1 (models agree), while human text has lower ratio (models disagree more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02023ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61733efe",
   "metadata": {},
   "source": [
    "## Step 1: Load Models\n",
    "\n",
    "We need two models from the same model family:\n",
    "- **Observer**: The instruct-tuned version (fine-tuned for following instructions)\n",
    "- **Performer**: The base pretrained version (not fine-tuned)\n",
    "\n",
    "Both models are loaded in fp16 for efficiency and set to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6004e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (MPS for Mac, or use \"cuda\" for NVIDIA GPUs, \"cpu\" for CPU)\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Load tokenizer (shared by both models)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Observer: Instruct-tuned model (the \"judge\")\n",
    "observer = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Performer: Base pretrained model (the \"predictor\")\n",
    "performer = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Move models to device and set to evaluation mode\n",
    "observer.to(device)\n",
    "performer.to(device)\n",
    "observer.eval()\n",
    "performer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b449ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Implement Core Metrics\n",
    "\n",
    "# ========== PERPLEXITY (PPL) ==========\n",
    "def calculate_PPL(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity using the OBSERVER model.\n",
    "    \n",
    "    Perplexity measures how \"surprised\" the model is by the text.\n",
    "    Lower PPL = text is more predictable to the model.\n",
    "    \n",
    "    PPL = exp(average negative log-likelihood of the true tokens)\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens.get(\"attention_mask\", torch.ones_like(input_ids)).to(device)\n",
    "\n",
    "    observer.to(device)\n",
    "    observer.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get logits from the observer model\n",
    "        outputs = observer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Shift for next-token prediction: predict token i+1 given tokens 0..i\n",
    "        shift_logits = logits[:, :-1, :]  # Remove last token's logits\n",
    "        shift_labels = input_ids[:, 1:]  # Remove first token (nothing to predict)\n",
    "        shift_mask = attention_mask[:, 1:]\n",
    "\n",
    "        # Convert logits to log probabilities\n",
    "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Get log prob of the ACTUAL next token\n",
    "        gathered_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Compute average negative log-likelihood (masked by attention)\n",
    "        masked_nll = -gathered_log_probs * shift_mask\n",
    "        total_tokens = shift_mask.sum().clamp_min(1)\n",
    "        mean_nll = masked_nll.sum() / total_tokens\n",
    "\n",
    "        # Perplexity = exp(NLL)\n",
    "        ppl = torch.exp(mean_nll).item()\n",
    "        return float(ppl)\n",
    "\n",
    "\n",
    "# ========== CROSS-PERPLEXITY (X-PPL) ==========\n",
    "def calculate_X_PPL(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cross-perplexity: how surprised the OBSERVER is by what the PERFORMER predicts.\n",
    "    \n",
    "    Key difference from PPL:\n",
    "    - PPL: observer evaluates the ACTUAL next tokens\n",
    "    - X-PPL: observer evaluates the tokens the PERFORMER would choose\n",
    "    \n",
    "    This measures agreement between the two models.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens.get(\"attention_mask\", torch.ones_like(input_ids)).to(device)\n",
    "\n",
    "    observer.to(device)\n",
    "    performer.to(device)\n",
    "    observer.eval()\n",
    "    performer.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get predictions from BOTH models\n",
    "        obs_outputs = observer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        perf_outputs = performer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        obs_logits = obs_outputs.logits\n",
    "        perf_logits = perf_outputs.logits\n",
    "\n",
    "        # Shift for next-token prediction\n",
    "        obs_shift_logits = obs_logits[:, :-1, :]\n",
    "        perf_shift_logits = perf_logits[:, :-1, :]\n",
    "        shift_mask = attention_mask[:, 1:]\n",
    "\n",
    "        # Get the tokens the PERFORMER would predict (argmax)\n",
    "        performer_pred_tokens = perf_shift_logits.argmax(dim=-1)\n",
    "\n",
    "        # Evaluate those predictions using the OBSERVER's probabilities\n",
    "        obs_log_probs = torch.log_softmax(obs_shift_logits, dim=-1)\n",
    "        gathered_log_probs = obs_log_probs.gather(dim=-1, index=performer_pred_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Compute average negative log-likelihood\n",
    "        masked_nll = -gathered_log_probs * shift_mask\n",
    "        total_positions = shift_mask.sum().clamp_min(1)\n",
    "        mean_nll = masked_nll.sum() / total_positions\n",
    "\n",
    "        # Cross-perplexity = exp(NLL)\n",
    "        xppl = torch.exp(mean_nll).item()\n",
    "        return float(xppl)\n",
    "\n",
    "\n",
    "# ========== BINOCULARS SCORE ==========\n",
    "def binoculars_score(text: str):\n",
    "    \"\"\"\n",
    "    Compute the Binoculars detection score.\n",
    "    \n",
    "    The key metric is the RATIO = X-PPL / PPL:\n",
    "    - Ratio close to 1.0: Models agree → likely AI-generated\n",
    "    - Ratio < 1.0: Models disagree → likely human-written\n",
    "    \n",
    "    Returns a dictionary with all metrics and a simple threshold-based prediction.\n",
    "    \"\"\"\n",
    "    ppl = calculate_PPL(text)\n",
    "    xppl = calculate_X_PPL(text)\n",
    "    \n",
    "    # Compute the ratio (avoid division by zero)\n",
    "    ratio = xppl / max(ppl, 1e-9)\n",
    "    \n",
    "    # Simple threshold: ratio > 1.0 suggests AI-generated\n",
    "    is_ai = ratio > 1.0\n",
    "    \n",
    "    return {\"ppl\": ppl, \"xppl\": xppl, \"ratio\": ratio, \"is_ai\": is_ai}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb72e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model text:\n",
      " Write two sentences about the importance of reading. What are some of the benefits of reading?\n",
      "What are some of the benefits of reading?\n",
      "Reading is a very important skill that can help you in many ways. Some of the benefits of reading are that \n",
      "\n",
      "HUMAN? {'ppl': 16.515625, 'xppl': 7.33984375, 'ratio': 0.44441816461684014, 'is_ai': False}\n",
      "MODEL? {'ppl': 5.546875, 'xppl': 4.13671875, 'ratio': 0.745774647887324, 'is_ai': False}\n"
     ]
    }
   ],
   "source": [
    "## Step 3: Minimal Demo\n",
    "\n",
    "# Example human-written text\n",
    "human_text = \"I took a long walk by the river this morning, and the air smelled like rain.\"\n",
    "\n",
    "# Generate AI text using the performer model\n",
    "prompt = \"Write two sentences about the importance of reading.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    gen_ids = performer.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=40,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "model_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Model text:\\n\", model_text, \"\\n\")\n",
    "\n",
    "# Compare the two texts using Binoculars\n",
    "for label, text in [(\"HUMAN?\", human_text), (\"MODEL?\", model_text)]:\n",
    "    metrics = binoculars_score(text)\n",
    "    print(label, metrics)\n",
    "\n",
    "# Expected pattern:\n",
    "# - Human text: Lower ratio (models disagree more)\n",
    "# - AI text: Higher ratio (models agree more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f0d88f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 01 | human: score=0.510 | ai: score=0.774 | Predicted AI=ai | Correct=True\n",
      "Pair 02 | human: score=0.352 | ai: score=0.681 | Predicted AI=ai | Correct=True\n",
      "Pair 03 | human: score=0.388 | ai: score=0.910 | Predicted AI=ai | Correct=True\n",
      "Pair 04 | human: score=0.327 | ai: score=0.748 | Predicted AI=ai | Correct=True\n",
      "Pair 05 | human: score=0.376 | ai: score=0.679 | Predicted AI=ai | Correct=True\n",
      "Pair 06 | human: score=0.352 | ai: score=0.753 | Predicted AI=ai | Correct=True\n",
      "Pair 07 | human: score=0.361 | ai: score=0.691 | Predicted AI=ai | Correct=True\n",
      "Pair 08 | human: score=0.321 | ai: score=1.026 | Predicted AI=ai | Correct=True\n",
      "Pair 09 | human: score=0.751 | ai: score=0.611 | Predicted AI=human | Correct=False\n",
      "Pair 10 | human: score=0.524 | ai: score=0.609 | Predicted AI=ai | Correct=True\n",
      "Pair 11 | human: score=0.522 | ai: score=0.686 | Predicted AI=ai | Correct=True\n",
      "Pair 12 | human: score=0.558 | ai: score=0.676 | Predicted AI=ai | Correct=True\n",
      "Pair 13 | human: score=0.631 | ai: score=0.557 | Predicted AI=human | Correct=False\n",
      "Pair 14 | human: score=0.648 | ai: score=0.845 | Predicted AI=ai | Correct=True\n",
      "Pair 15 | human: score=0.432 | ai: score=0.652 | Predicted AI=ai | Correct=True\n",
      "Pair 16 | human: score=0.706 | ai: score=0.601 | Predicted AI=human | Correct=False\n",
      "Pair 17 | human: score=0.551 | ai: score=0.734 | Predicted AI=ai | Correct=True\n",
      "Pair 18 | human: score=0.313 | ai: score=0.753 | Predicted AI=ai | Correct=True\n",
      "Pair 19 | human: score=0.405 | ai: score=0.736 | Predicted AI=ai | Correct=True\n",
      "Pair 20 | human: score=0.495 | ai: score=0.813 | Predicted AI=ai | Correct=True\n",
      "Pair 21 | human: score=0.621 | ai: score=0.743 | Predicted AI=ai | Correct=True\n",
      "Pair 22 | human: score=0.451 | ai: score=0.638 | Predicted AI=ai | Correct=True\n",
      "Pair 23 | human: score=0.396 | ai: score=0.741 | Predicted AI=ai | Correct=True\n",
      "Pair 24 | human: score=0.373 | ai: score=0.937 | Predicted AI=ai | Correct=True\n",
      "Pair 25 | human: score=0.720 | ai: score=0.609 | Predicted AI=human | Correct=False\n",
      "Pair 26 | human: score=0.542 | ai: score=0.809 | Predicted AI=ai | Correct=True\n",
      "Pair 27 | human: score=0.233 | ai: score=0.645 | Predicted AI=ai | Correct=True\n",
      "Pair 28 | human: score=0.451 | ai: score=0.879 | Predicted AI=ai | Correct=True\n",
      "Pair 29 | human: score=0.633 | ai: score=0.850 | Predicted AI=ai | Correct=True\n",
      "Pair 30 | human: score=0.254 | ai: score=0.881 | Predicted AI=ai | Correct=True\n",
      "Pair 31 | human: score=0.443 | ai: score=0.612 | Predicted AI=ai | Correct=True\n",
      "Pair 32 | human: score=0.532 | ai: score=1.165 | Predicted AI=ai | Correct=True\n",
      "Pair 33 | human: score=0.378 | ai: score=0.820 | Predicted AI=ai | Correct=True\n",
      "Pair 34 | human: score=0.712 | ai: score=0.854 | Predicted AI=ai | Correct=True\n",
      "Pair 35 | human: score=0.650 | ai: score=0.554 | Predicted AI=human | Correct=False\n",
      "Pair 36 | human: score=0.492 | ai: score=0.860 | Predicted AI=ai | Correct=True\n",
      "Pair 37 | human: score=0.707 | ai: score=1.021 | Predicted AI=ai | Correct=True\n",
      "Pair 38 | human: score=0.360 | ai: score=0.807 | Predicted AI=ai | Correct=True\n",
      "Pair 39 | human: score=0.315 | ai: score=0.845 | Predicted AI=ai | Correct=True\n",
      "Pair 40 | human: score=0.438 | ai: score=0.677 | Predicted AI=ai | Correct=True\n",
      "Pair 41 | human: score=0.452 | ai: score=1.110 | Predicted AI=ai | Correct=True\n",
      "Pair 42 | human: score=0.321 | ai: score=0.848 | Predicted AI=ai | Correct=True\n",
      "Pair 43 | human: score=0.470 | ai: score=0.490 | Predicted AI=ai | Correct=True\n",
      "Pair 44 | human: score=0.341 | ai: score=0.540 | Predicted AI=ai | Correct=True\n",
      "Pair 45 | human: score=0.448 | ai: score=0.544 | Predicted AI=ai | Correct=True\n",
      "Pair 46 | human: score=0.405 | ai: score=0.611 | Predicted AI=ai | Correct=True\n",
      "Pair 47 | human: score=0.410 | ai: score=0.572 | Predicted AI=ai | Correct=True\n",
      "Pair 48 | human: score=0.415 | ai: score=0.778 | Predicted AI=ai | Correct=True\n",
      "Pair 49 | human: score=0.374 | ai: score=0.549 | Predicted AI=ai | Correct=True\n",
      "Pair 50 | human: score=0.388 | ai: score=0.614 | Predicted AI=ai | Correct=True\n",
      "Pair 51 | human: score=0.470 | ai: score=0.451 | Predicted AI=human | Correct=False\n",
      "Pair 52 | human: score=0.561 | ai: score=0.993 | Predicted AI=ai | Correct=True\n",
      "Pair 53 | human: score=0.417 | ai: score=0.947 | Predicted AI=ai | Correct=True\n",
      "Pair 54 | human: score=0.371 | ai: score=0.658 | Predicted AI=ai | Correct=True\n",
      "Pair 55 | human: score=0.362 | ai: score=0.523 | Predicted AI=ai | Correct=True\n",
      "Pair 56 | human: score=0.543 | ai: score=0.776 | Predicted AI=ai | Correct=True\n",
      "Pair 57 | human: score=0.358 | ai: score=0.769 | Predicted AI=ai | Correct=True\n",
      "Pair 58 | human: score=0.443 | ai: score=0.749 | Predicted AI=ai | Correct=True\n",
      "Pair 59 | human: score=0.383 | ai: score=0.746 | Predicted AI=ai | Correct=True\n",
      "Pair 60 | human: score=0.609 | ai: score=0.819 | Predicted AI=ai | Correct=True\n",
      "Pair 61 | human: score=0.360 | ai: score=0.697 | Predicted AI=ai | Correct=True\n",
      "Pair 62 | human: score=0.421 | ai: score=0.852 | Predicted AI=ai | Correct=True\n",
      "Pair 63 | human: score=0.672 | ai: score=0.695 | Predicted AI=ai | Correct=True\n",
      "Pair 64 | human: score=0.331 | ai: score=0.758 | Predicted AI=ai | Correct=True\n",
      "Pair 65 | human: score=0.641 | ai: score=0.925 | Predicted AI=ai | Correct=True\n",
      "Pair 66 | human: score=0.709 | ai: score=0.560 | Predicted AI=human | Correct=False\n",
      "Pair 67 | human: score=0.389 | ai: score=0.543 | Predicted AI=ai | Correct=True\n",
      "Pair 68 | human: score=0.439 | ai: score=0.932 | Predicted AI=ai | Correct=True\n",
      "Pair 69 | human: score=0.581 | ai: score=0.615 | Predicted AI=ai | Correct=True\n",
      "Pair 70 | human: score=0.440 | ai: score=0.878 | Predicted AI=ai | Correct=True\n",
      "Pair 71 | human: score=0.457 | ai: score=0.712 | Predicted AI=ai | Correct=True\n",
      "Pair 72 | human: score=0.490 | ai: score=0.648 | Predicted AI=ai | Correct=True\n",
      "Pair 73 | human: score=0.489 | ai: score=0.736 | Predicted AI=ai | Correct=True\n",
      "Pair 74 | human: score=0.664 | ai: score=0.804 | Predicted AI=ai | Correct=True\n",
      "Pair 75 | human: score=0.448 | ai: score=0.972 | Predicted AI=ai | Correct=True\n",
      "Pair 76 | human: score=0.278 | ai: score=0.585 | Predicted AI=ai | Correct=True\n",
      "Pair 77 | human: score=0.617 | ai: score=0.907 | Predicted AI=ai | Correct=True\n",
      "Pair 78 | human: score=0.470 | ai: score=0.754 | Predicted AI=ai | Correct=True\n",
      "Pair 79 | human: score=0.464 | ai: score=0.867 | Predicted AI=ai | Correct=True\n",
      "Pair 80 | human: score=0.555 | ai: score=0.489 | Predicted AI=human | Correct=False\n",
      "Pair 81 | human: score=0.327 | ai: score=0.306 | Predicted AI=human | Correct=False\n",
      "Pair 82 | human: score=0.486 | ai: score=0.826 | Predicted AI=ai | Correct=True\n",
      "Pair 83 | human: score=0.337 | ai: score=0.882 | Predicted AI=ai | Correct=True\n",
      "Pair 84 | human: score=0.434 | ai: score=0.713 | Predicted AI=ai | Correct=True\n",
      "Pair 85 | human: score=0.382 | ai: score=0.730 | Predicted AI=ai | Correct=True\n",
      "Pair 86 | human: score=0.558 | ai: score=0.686 | Predicted AI=ai | Correct=True\n",
      "Pair 87 | human: score=0.466 | ai: score=0.803 | Predicted AI=ai | Correct=True\n",
      "Pair 88 | human: score=0.400 | ai: score=0.740 | Predicted AI=ai | Correct=True\n",
      "Pair 89 | human: score=0.656 | ai: score=0.784 | Predicted AI=ai | Correct=True\n",
      "Pair 90 | human: score=0.314 | ai: score=0.721 | Predicted AI=ai | Correct=True\n",
      "Pair 91 | human: score=0.447 | ai: score=0.661 | Predicted AI=ai | Correct=True\n",
      "Pair 92 | human: score=0.927 | ai: score=0.699 | Predicted AI=human | Correct=False\n",
      "Pair 93 | human: score=0.418 | ai: score=0.818 | Predicted AI=ai | Correct=True\n",
      "Pair 94 | human: score=0.309 | ai: score=0.797 | Predicted AI=ai | Correct=True\n",
      "Pair 95 | human: score=0.565 | ai: score=0.724 | Predicted AI=ai | Correct=True\n",
      "Pair 96 | human: score=0.345 | ai: score=0.741 | Predicted AI=ai | Correct=True\n",
      "Pair 97 | human: score=0.512 | ai: score=0.803 | Predicted AI=ai | Correct=True\n",
      "Pair 98 | human: score=0.431 | ai: score=0.612 | Predicted AI=ai | Correct=True\n",
      "Pair 99 | human: score=0.509 | ai: score=0.789 | Predicted AI=ai | Correct=True\n",
      "Pair 100 | human: score=0.544 | ai: score=0.689 | Predicted AI=ai | Correct=True\n",
      "Pair 101 | human: score=0.455 | ai: score=0.933 | Predicted AI=ai | Correct=True\n",
      "Pair 102 | human: score=0.398 | ai: score=0.643 | Predicted AI=ai | Correct=True\n",
      "Pair 103 | human: score=0.553 | ai: score=0.785 | Predicted AI=ai | Correct=True\n",
      "Pair 104 | human: score=0.428 | ai: score=0.971 | Predicted AI=ai | Correct=True\n",
      "Pair 105 | human: score=0.464 | ai: score=0.696 | Predicted AI=ai | Correct=True\n",
      "Pair 106 | human: score=0.409 | ai: score=0.821 | Predicted AI=ai | Correct=True\n",
      "Pair 107 | human: score=0.343 | ai: score=0.728 | Predicted AI=ai | Correct=True\n",
      "Pair 108 | human: score=0.388 | ai: score=0.852 | Predicted AI=ai | Correct=True\n",
      "Pair 109 | human: score=0.395 | ai: score=0.785 | Predicted AI=ai | Correct=True\n",
      "Pair 110 | human: score=0.893 | ai: score=0.622 | Predicted AI=human | Correct=False\n",
      "Pair 111 | human: score=0.393 | ai: score=0.460 | Predicted AI=ai | Correct=True\n",
      "Pair 112 | human: score=0.434 | ai: score=0.790 | Predicted AI=ai | Correct=True\n",
      "Pair 113 | human: score=0.462 | ai: score=0.842 | Predicted AI=ai | Correct=True\n",
      "Pair 114 | human: score=0.352 | ai: score=0.886 | Predicted AI=ai | Correct=True\n",
      "Pair 115 | human: score=0.339 | ai: score=0.740 | Predicted AI=ai | Correct=True\n",
      "Pair 116 | human: score=0.543 | ai: score=0.695 | Predicted AI=ai | Correct=True\n",
      "Pair 117 | human: score=0.524 | ai: score=0.773 | Predicted AI=ai | Correct=True\n",
      "Pair 118 | human: score=0.516 | ai: score=0.766 | Predicted AI=ai | Correct=True\n",
      "Pair 119 | human: score=0.803 | ai: score=0.631 | Predicted AI=human | Correct=False\n",
      "Pair 120 | human: score=0.435 | ai: score=0.736 | Predicted AI=ai | Correct=True\n",
      "Pair 121 | human: score=0.428 | ai: score=0.708 | Predicted AI=ai | Correct=True\n",
      "Pair 122 | human: score=0.448 | ai: score=0.733 | Predicted AI=ai | Correct=True\n",
      "Pair 123 | human: score=0.474 | ai: score=0.565 | Predicted AI=ai | Correct=True\n",
      "Pair 124 | human: score=0.894 | ai: score=0.816 | Predicted AI=human | Correct=False\n",
      "Pair 125 | human: score=0.492 | ai: score=0.769 | Predicted AI=ai | Correct=True\n",
      "Pair 126 | human: score=0.506 | ai: score=0.817 | Predicted AI=ai | Correct=True\n",
      "Pair 127 | human: score=0.537 | ai: score=0.746 | Predicted AI=ai | Correct=True\n",
      "Pair 128 | human: score=0.347 | ai: score=0.702 | Predicted AI=ai | Correct=True\n",
      "Pair 129 | human: score=0.421 | ai: score=0.436 | Predicted AI=ai | Correct=True\n",
      "Pair 130 | human: score=0.471 | ai: score=0.626 | Predicted AI=ai | Correct=True\n",
      "Pair 131 | human: score=0.338 | ai: score=0.774 | Predicted AI=ai | Correct=True\n",
      "Pair 132 | human: score=0.306 | ai: score=0.730 | Predicted AI=ai | Correct=True\n",
      "Pair 133 | human: score=0.526 | ai: score=0.851 | Predicted AI=ai | Correct=True\n",
      "Pair 134 | human: score=0.469 | ai: score=0.714 | Predicted AI=ai | Correct=True\n",
      "Pair 135 | human: score=0.425 | ai: score=1.002 | Predicted AI=ai | Correct=True\n",
      "Pair 136 | human: score=0.314 | ai: score=0.866 | Predicted AI=ai | Correct=True\n",
      "Pair 137 | human: score=0.480 | ai: score=0.779 | Predicted AI=ai | Correct=True\n",
      "Pair 138 | human: score=0.449 | ai: score=0.619 | Predicted AI=ai | Correct=True\n",
      "Pair 139 | human: score=0.297 | ai: score=0.633 | Predicted AI=ai | Correct=True\n",
      "Pair 140 | human: score=0.952 | ai: score=0.753 | Predicted AI=human | Correct=False\n",
      "Pair 141 | human: score=0.357 | ai: score=0.788 | Predicted AI=ai | Correct=True\n",
      "Pair 142 | human: score=0.488 | ai: score=0.636 | Predicted AI=ai | Correct=True\n",
      "Pair 143 | human: score=0.681 | ai: score=0.732 | Predicted AI=ai | Correct=True\n",
      "Pair 144 | human: score=0.427 | ai: score=0.829 | Predicted AI=ai | Correct=True\n",
      "Pair 145 | human: score=0.496 | ai: score=0.756 | Predicted AI=ai | Correct=True\n",
      "Pair 146 | human: score=0.359 | ai: score=0.640 | Predicted AI=ai | Correct=True\n",
      "Pair 147 | human: score=0.496 | ai: score=0.770 | Predicted AI=ai | Correct=True\n",
      "Pair 148 | human: score=0.338 | ai: score=0.774 | Predicted AI=ai | Correct=True\n",
      "Pair 149 | human: score=0.525 | ai: score=0.812 | Predicted AI=ai | Correct=True\n",
      "Pair 150 | human: score=0.482 | ai: score=0.941 | Predicted AI=ai | Correct=True\n",
      "Pair 151 | human: score=0.361 | ai: score=0.957 | Predicted AI=ai | Correct=True\n",
      "Pair 152 | human: score=0.478 | ai: score=0.674 | Predicted AI=ai | Correct=True\n",
      "Pair 153 | human: score=0.919 | ai: score=0.547 | Predicted AI=human | Correct=False\n",
      "Pair 154 | human: score=0.365 | ai: score=1.090 | Predicted AI=ai | Correct=True\n",
      "Pair 155 | human: score=0.753 | ai: score=0.752 | Predicted AI=human | Correct=False\n",
      "Pair 156 | human: score=0.491 | ai: score=0.655 | Predicted AI=ai | Correct=True\n",
      "Pair 157 | human: score=0.550 | ai: score=0.949 | Predicted AI=ai | Correct=True\n",
      "Pair 158 | human: score=0.465 | ai: score=0.716 | Predicted AI=ai | Correct=True\n",
      "Pair 159 | human: score=0.375 | ai: score=0.790 | Predicted AI=ai | Correct=True\n",
      "Pair 160 | human: score=0.364 | ai: score=0.851 | Predicted AI=ai | Correct=True\n",
      "Pair 161 | human: score=0.268 | ai: score=0.928 | Predicted AI=ai | Correct=True\n",
      "Pair 162 | human: score=0.439 | ai: score=0.926 | Predicted AI=ai | Correct=True\n",
      "Pair 163 | human: score=0.647 | ai: score=0.596 | Predicted AI=human | Correct=False\n",
      "Pair 164 | human: score=0.502 | ai: score=0.421 | Predicted AI=human | Correct=False\n",
      "Pair 165 | human: score=0.507 | ai: score=0.609 | Predicted AI=ai | Correct=True\n",
      "Pair 166 | human: score=0.542 | ai: score=0.721 | Predicted AI=ai | Correct=True\n",
      "Pair 167 | human: score=0.291 | ai: score=0.670 | Predicted AI=ai | Correct=True\n",
      "Pair 168 | human: score=0.374 | ai: score=0.480 | Predicted AI=ai | Correct=True\n",
      "Pair 169 | human: score=0.598 | ai: score=0.943 | Predicted AI=ai | Correct=True\n",
      "Pair 170 | human: score=0.441 | ai: score=0.760 | Predicted AI=ai | Correct=True\n",
      "Pair 171 | human: score=0.858 | ai: score=0.745 | Predicted AI=human | Correct=False\n",
      "Pair 172 | human: score=0.410 | ai: score=0.474 | Predicted AI=ai | Correct=True\n",
      "Pair 173 | human: score=0.386 | ai: score=0.594 | Predicted AI=ai | Correct=True\n",
      "Pair 174 | human: score=0.393 | ai: score=0.719 | Predicted AI=ai | Correct=True\n",
      "Pair 175 | human: score=0.329 | ai: score=0.711 | Predicted AI=ai | Correct=True\n",
      "Pair 176 | human: score=0.519 | ai: score=0.681 | Predicted AI=ai | Correct=True\n",
      "Pair 177 | human: score=0.430 | ai: score=0.685 | Predicted AI=ai | Correct=True\n",
      "Pair 178 | human: score=0.502 | ai: score=0.864 | Predicted AI=ai | Correct=True\n",
      "Pair 179 | human: score=0.496 | ai: score=0.737 | Predicted AI=ai | Correct=True\n",
      "Pair 180 | human: score=0.712 | ai: score=0.817 | Predicted AI=ai | Correct=True\n",
      "Pair 181 | human: score=0.386 | ai: score=0.605 | Predicted AI=ai | Correct=True\n",
      "Pair 182 | human: score=0.332 | ai: score=0.696 | Predicted AI=ai | Correct=True\n",
      "Pair 183 | human: score=0.444 | ai: score=0.726 | Predicted AI=ai | Correct=True\n",
      "Pair 184 | human: score=0.444 | ai: score=0.776 | Predicted AI=ai | Correct=True\n",
      "Pair 185 | human: score=0.422 | ai: score=0.557 | Predicted AI=ai | Correct=True\n",
      "Pair 186 | human: score=0.508 | ai: score=0.648 | Predicted AI=ai | Correct=True\n",
      "Pair 187 | human: score=0.490 | ai: score=0.836 | Predicted AI=ai | Correct=True\n",
      "Pair 188 | human: score=0.508 | ai: score=0.702 | Predicted AI=ai | Correct=True\n",
      "Pair 189 | human: score=0.591 | ai: score=0.899 | Predicted AI=ai | Correct=True\n",
      "Pair 190 | human: score=0.391 | ai: score=0.913 | Predicted AI=ai | Correct=True\n",
      "Pair 191 | human: score=0.576 | ai: score=0.736 | Predicted AI=ai | Correct=True\n",
      "Pair 192 | human: score=0.365 | ai: score=0.698 | Predicted AI=ai | Correct=True\n",
      "Pair 193 | human: score=0.867 | ai: score=0.577 | Predicted AI=human | Correct=False\n",
      "Pair 194 | human: score=0.367 | ai: score=0.729 | Predicted AI=ai | Correct=True\n",
      "Pair 195 | human: score=0.586 | ai: score=0.681 | Predicted AI=ai | Correct=True\n",
      "Pair 196 | human: score=0.492 | ai: score=0.617 | Predicted AI=ai | Correct=True\n",
      "Pair 197 | human: score=0.678 | ai: score=0.623 | Predicted AI=human | Correct=False\n",
      "Pair 198 | human: score=0.390 | ai: score=0.757 | Predicted AI=ai | Correct=True\n",
      "Pair 199 | human: score=0.470 | ai: score=0.988 | Predicted AI=ai | Correct=True\n",
      "Pair 200 | human: score=0.418 | ai: score=0.745 | Predicted AI=ai | Correct=True\n",
      "\n",
      "Processed pairs: 200\n",
      "Accuracy (AI = higher ratio): 89.5%\n",
      "\n",
      "Key observation: AI-generated text typically has a HIGHER X-PPL/PPL ratio\n",
      "because the observer and performer models agree more on AI text.\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Evaluate on Real Dataset\n",
    "\n",
    "# Load human-AI text pairs from JSONL dataset and evaluate accuracy\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to dataset containing human responses and AI-generated responses\n",
    "jsonl_path = Path(\"/Users/mihirmishra/CodingStuff/Projects/Models-from-scratch/Binoculars/no-robotic-words-llama2-13b-chat.jsonl\")\n",
    "\n",
    "num_pairs = 200  # Number of pairs to evaluate\n",
    "processed = 0\n",
    "correct = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "if not jsonl_path.exists():\n",
    "    raise FileNotFoundError(f\"JSONL not found: {jsonl_path}\")\n",
    "\n",
    "with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line_idx, line in enumerate(f):\n",
    "        if processed >= num_pairs:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # Extract human and AI texts from the dataset\n",
    "        human_text = rec.get(\"response\")\n",
    "        ai_text = rec.get(\"meta-llama-Llama-2-13b-chat-hf_generated_text_wo_prompt\")\n",
    "        if not human_text or not ai_text:\n",
    "            continue\n",
    "\n",
    "        # Compute Binoculars metrics for both texts\n",
    "        human_metrics = binoculars_score(human_text)\n",
    "        ai_metrics = binoculars_score(ai_text)\n",
    "\n",
    "        # Prediction: The text with HIGHER ratio is predicted as AI\n",
    "        # (because higher ratio means the two models agree more)\n",
    "        predicted_ai = \"ai\" if ai_metrics[\"ratio\"] >= human_metrics[\"ratio\"] else \"human\"\n",
    "        is_correct = (predicted_ai == \"ai\")\n",
    "\n",
    "        processed += 1\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        results.append({\n",
    "            \"idx\": processed,\n",
    "            \"human\": human_metrics,\n",
    "            \"ai\": ai_metrics,\n",
    "            \"predicted_ai\": predicted_ai,\n",
    "            \"correct\": is_correct,\n",
    "        })\n",
    "\n",
    "# Print detailed results for each pair\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"Pair {r['idx']:02d} | \"\n",
    "        f\"human: score={r['human']['ratio']:.3f} | \"\n",
    "        f\"ai: score={r['ai']['ratio']:.3f} | \"\n",
    "        f\"Predicted AI={r['predicted_ai']} | Correct={r['correct']}\"\n",
    "    )\n",
    "\n",
    "# Print overall accuracy\n",
    "if processed == 0:\n",
    "    print(\"No valid pairs processed.\")\n",
    "else:\n",
    "    acc = correct / processed\n",
    "    print(\"\")\n",
    "    print(f\"Processed pairs: {processed}\")\n",
    "    print(f\"Accuracy (AI = higher ratio): {acc:.1%}\")\n",
    "    print(\"\")\n",
    "    print(\"Key observation: AI-generated text typically has a HIGHER X-PPL/PPL ratio\")\n",
    "    print(\"because the observer and performer models agree more on AI text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41adc1",
   "metadata": {},
   "source": [
    "## Step 5: Test on Custom Text\n",
    "\n",
    "Let's test the Binoculars detector on a new piece of text—the first message from Yann in the Slack channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94dfcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL (Perplexity): 25.234\n",
      "X-PPL (Cross-Perplexity): 3.777\n",
      "Ratio (X-PPL/PPL): 0.150\n",
      "Predicted as AI: False\n",
      "\n",
      "✓ Strong indication: HUMAN-WRITTEN (very low ratio)\n"
     ]
    }
   ],
   "source": [
    "# Test text - Slack message\n",
    "test_text = \"\"\"Hello @channel,\n",
    "Excited to kick off the semester!\n",
    "A couple of announcements and reminders:\n",
    "1. Profile pictures\n",
    "Please update your Slack profile picture so Kilian and I can more easily connect names to faces.\n",
    "2. Lecture outlines - content\n",
    "Reach out to me at least 2 weeks before your teaching date with your outline. This outline should be fairly fleshed out. It should be the output of your blogpost / existing online lectures review. I'll then share it with Kilian so we can make sure the content is comprehensive, up to date, and accurate. We can talk about it during office hours on Fridays as well.\n",
    "3. Rehearsals - format\n",
    "One week before your class (on Fridays), you'll do a final rehearsal with me. We'll go over format, flow, and ensure none of the deadly sins have crept in so you can shine as engaging lecturers who captivate your audience.\n",
    "Looking forward to working with you all!\"\"\"\n",
    "\n",
    "# Run Binoculars detection\n",
    "result = binoculars_score(test_text)\n",
    "\n",
    "print(f\"PPL (Perplexity): {result['ppl']:.3f}\")\n",
    "print(f\"X-PPL (Cross-Perplexity): {result['xppl']:.3f}\")\n",
    "print(f\"Ratio (X-PPL/PPL): {result['ratio']:.3f}\")\n",
    "print(f\"Predicted as AI: {result['is_ai']}\")\n",
    "print()\n",
    "\n",
    "# Interpret the result\n",
    "if result['ratio'] < 0.7:\n",
    "    print(\"Strong indication: HUMAN-WRITTEN (very low ratio)\")\n",
    "elif result['ratio'] < 1.0:\n",
    "    print(\"Likely: HUMAN-WRITTEN (ratio < 1.0)\")\n",
    "else:\n",
    "    print(\"Likely: AI-GENERATED (ratio >= 1.0)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
